{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from nn.nn import NeuralNetwork\n",
    "import nn\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read in and process data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from nn import io\n",
    "\n",
    "# Read in positive sequences\n",
    "pos_seqs = io.read_text_file('data/rap1-lieb-positives.txt')\n",
    "\n",
    "# Read in negative sequences\n",
    "neg_seqs = io.read_fasta_file('data/yeast-upstream-1k-negative.fa')\n",
    "\n",
    "# Process data so that negative and positive sequences are the same length\n",
    "\n",
    "# Initialize a list to store processed sequences\n",
    "neg_seqs_processed = []\n",
    "\n",
    "# Positive sequences are shorter than negative sequences\n",
    "# so set shorter length variable to the length of the positive sequences\n",
    "pos_seq_len = len(pos_seqs[0])\n",
    "\n",
    "# For each of the negative sequences, iterate through the sequence and\n",
    "# return a subsequence with the same length as the positive sequences.\n",
    "# Then, store that subsequence in the seqs_processed list\n",
    "for seq in neg_seqs:\n",
    "    for i in range(len(seq) - pos_seq_len + 1):\n",
    "        sub_seq = seq[i:i + pos_seq_len]\n",
    "        neg_seqs_processed.append(sub_seq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine positive and negative sequences and generate labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from nn import preprocess\n",
    "seqs = pos_seqs + neg_seqs_processed\n",
    "pos_labels = [1] * len(pos_seqs)\n",
    "neg_labels = [0] * len(neg_seqs_processed)\n",
    "labels = pos_labels + neg_labels\n",
    "\n",
    "samples, sample_labels = preprocess.sample_seqs(seqs, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate a one-hot encoding of sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "X = preprocess.one_hot_encode_seqs(samples)\n",
    "y = np.array(sample_labels, dtype=int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data into training and validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create an instance of the NeuralNetwork class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# CreatÐµ an instance of the NeuralNetwork class with defined hyperparameters\n",
    "\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 68, \"output_dim\": 32, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 32, \"output_dim\": 16, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n",
    "# Create NeuralNetwork instance\n",
    "nn = NeuralNetwork(nn_arch, lr=0.01, seed=42, batch_size=16, epochs=1000, loss_function=\"binary cross entropy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Train the neural network\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m training_loss, validation_loss \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Winter_2023/BMI_203/final-nn/nn/nn.py:323\u001B[0m, in \u001B[0;36mNeuralNetwork.fit\u001B[0;34m(self, X_train, y_train, X_val, y_val)\u001B[0m\n\u001B[1;32m    321\u001B[0m     y_batch \u001B[38;5;241m=\u001B[39m y_batch\u001B[38;5;241m.\u001B[39mT  \u001B[38;5;66;03m# Transpose each X batch\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     y_hat, cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(X_batch)  \u001B[38;5;66;03m# Forward pass on X batches\u001B[39;00m\n\u001B[0;32m--> 323\u001B[0m     grad_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackprop\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_hat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Backpropagate using y_batches\u001B[39;00m\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_params(grad_dict)  \u001B[38;5;66;03m# Update parameters using values stored from backpropagation\u001B[39;00m\n\u001B[1;32m    326\u001B[0m \u001B[38;5;66;03m# Calculate predictions from training and validation sets\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Winter_2023/BMI_203/final-nn/nn/nn.py:230\u001B[0m, in \u001B[0;36mNeuralNetwork.backprop\u001B[0;34m(self, y, y_hat, cache)\u001B[0m\n\u001B[1;32m    228\u001B[0m     dA_prev \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_binary_cross_entropy_backprop(y, y_hat)\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean squared error\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss_func:\n\u001B[0;32m--> 230\u001B[0m     dA_prev \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mean_squared_error_backprop\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_hat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss function name is not defined. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    233\u001B[0m                     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss function must be binary cross entropy or mean squared error.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/Winter_2023/BMI_203/final-nn/nn/nn.py:489\u001B[0m, in \u001B[0;36mNeuralNetwork._mean_squared_error_backprop\u001B[0;34m(self, y, y_hat)\u001B[0m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;124;03mMean square error loss derivative for backprop.\u001B[39;00m\n\u001B[1;32m    477\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m \u001B[38;5;124;03m        partial derivative of loss with respect to A matrix.\u001B[39;00m\n\u001B[1;32m    487\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;66;03m# Derivative of the MSE loss function\u001B[39;00m\n\u001B[0;32m--> 489\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m) \u001B[38;5;241m*\u001B[39m (y_hat\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m-\u001B[39m y)\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "training_loss, validation_loss = nn.fit(X_train, y_train, X_val, y_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot training and validation loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Plot training and validation loss by epoch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43mtraining_loss\u001B[49m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining Loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(validation_loss, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation Loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'training_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training and validation loss by epoch\n",
    "plt.plot(training_loss, label=\"Training Loss\")\n",
    "plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Average Training Error: {np.mean(training_loss)}')\n",
    "print(f'Average Validation Error: {np.mean(validation_loss)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter choice rationale:\n",
    "\n",
    "### Dimensions\n",
    "The input dimension was selected to be 68 since each nucleotide's encoding is represented by a 1x4 vector\n",
    "and each sequence of nucleotides is 17-nt in length (68 = 4 x 17). The output dimension was selected to be 1 since we are looking to do binary classification.\n",
    "\n",
    "### Learning rate\n",
    "The learning rate lr is set to 0.01, since that seems to be a common choice for gradient descent optimization. Decreasing the learning rate would make training go slower, but also risks getting stuck at a local minimum. Conversely, increasing the learning rate would allow model training to proceed more quickly but risks divergent behavior.\n",
    "\n",
    "### Seed value\n",
    "The random seed is set to 42 because, according to the supercomputer Deep Thought, the number 42 is the \"Answer to the Ultimate Question of Life, The Universe, and Everything.\"\n",
    "\n",
    "### Batch size\n",
    "The batch size is set to 16, because it seemed to be a reasonable size for this dataset.\n",
    "\n",
    "### Epoch number\n",
    "The number of epochs is set to 1000, because I was planning to see how the model performs at 1000 epochs and see how the error changes as I decrease the number of epochs the model runs through."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choice of loss function\n",
    "The loss function was selected to be the binary cross entropy function because it is well suited for binary classification problems such as this as it compares each of the predicted values to the ground truth values and returns a value of 0 or 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comments\n",
    "\n",
    "My autoencoder runs using either loss function, but I think there is an issue in how I read in or segment the data I train on for the classifier, since now I run into issues when I try to use either loss function. Specifically, I run into an issue where y.shape[1] is out of range for this dataset but not for the digits dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
